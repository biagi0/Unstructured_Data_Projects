{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment One - Analytics for Unstructured Data\n",
    "#### Team Members: Haden Loveridge, Mikala Lowrance, Arantza Garcia Delfin, Neha R Boinapalli, Lindsay Bartol, Sameer Kahn, Biagio Alessandrello "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 10 Brands we chose - Frequency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Show all lift values in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - MDS Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - State the 5 attributes you chose (again, a frequency table is good here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - For task F, provide all details of your analysis - e.g. how you measured \"aspirational\" and how you found the most aspirational brand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Advice/Insights based on your analysis for your client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --------------- Code ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_page(page_number):\n",
    "    url = f\"https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{page_number}\"\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find(id='vanilla_discussion_index')\n",
    "    print(f\"-- Scraping {url}\")\n",
    "\n",
    "    # GET ALL THE COMMENTS STARTING FROM THE LATEST\n",
    "    forum_posts = results.find_all(\"div\", class_=\"Comment\")\n",
    "    forum_posts.reverse()\n",
    "\n",
    "    rows = []\n",
    "    for post in forum_posts:\n",
    "        post_date = post.find(\"span\", class_=\"MItem DateCreated\")\n",
    "        post_comment = post.find(\"div\", class_=\"Message userContent\")\n",
    "\n",
    "        # REMOVE QUOTED REPLIES, IF THEY EXIST\n",
    "        if post_comment.find(\"blockquote\"):\n",
    "            post_comment.find(\"blockquote\").decompose()\n",
    "\n",
    "        row = [post_date.text.strip(), post_comment.text.strip()]\n",
    "        rows.append(row)\n",
    "\n",
    "    print(f\"---- Got {len(rows)} comments\")\n",
    "    return rows\n",
    "\n",
    "\n",
    "def scrape_pages(page_number=435, target_comment_count=5000):\n",
    "    comments = []\n",
    "\n",
    "    while len(comments) <= target_comment_count:\n",
    "        print(f\"- Total comments so far: {len(comments)}\")\n",
    "        page_data = scrape_page(page_number)\n",
    "        comments.extend(page_data)\n",
    "        page_number -= 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "def print_csv(rows, filename='edmunds_comments'):\n",
    "    with open(f'{filename}.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        writer.writerow([\"Date\", \"Comment\"])\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def run():\n",
    "    data = scrape_pages()\n",
    "    print_csv(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import io\n",
    "\n",
    "#gets rid of word frequency definition\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input and output filenames\n",
    "input_filename = 'edmunds_comments.csv' # Input file\n",
    "final_filename = 'A_mid.csv' # Intermediate file without the column header\n",
    "word_freq_output = 'A_word_freq.csv' # Output file for word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize sentences\n",
    "def clean_and_tokenize(sentence):\n",
    "    \"\"\"\n",
    "    Cleans a given sentence by removing punctuation and stopwords, converting text\n",
    "    to lowercase,\n",
    "    and tokenizing the remaining words.\n",
    "    \"\"\"\n",
    "    # Remove punctuation and convert text to lowercase\n",
    "    sentence = re.sub(f'[{re.escape(string.punctuation)}]', '', sentence.lower())\n",
    "    # Tokenize and remove stopwords\n",
    "    return sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove header from the input CSV and create a new file without it\n",
    "def remove_header(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads the input CSV file, removes the header, and writes the remaining rows\n",
    "    into a new output file.\n",
    "    \"\"\"\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        next(infile) #skips the header\n",
    "        outfile.writelines(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUR SAMPLE FILE HAS THE COMMENTS IN THE SECOND COLUMN NOT THE THIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Extract and clean sentences from the text\n",
    "def extract_sentences(file):\n",
    "    \"\"\"\n",
    "    Extracts text data from the second column of the CSV file, splits it into\n",
    "    sentences,\n",
    "    and cleans each sentence by removing punctuation and stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    sentences_clean = []\n",
    "\n",
    "    with open(file, mode='r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        \n",
    "        # Iterate over rows in the CSV file\n",
    "        for row in reader:\n",
    "            # Assuming the text is in the third column (index 1)\n",
    "            if len(row) > 1:\n",
    "                text = row[1]\n",
    "                \n",
    "                # Split the text into sentences based on punctuation (.?!)\n",
    "                sentences = re.split(r'[.!?]', text)\n",
    "                \n",
    "                # Clean and tokenize each sentence\n",
    "                for sentence in sentences:\n",
    "                    cleaned_tokens = clean_and_tokenize(sentence)\n",
    "                    if cleaned_tokens:  # Avoid adding empty sentences\n",
    "                        sentences_clean.extend(cleaned_tokens)\n",
    "                        #sentences_clean.append(cleaned_tokens)\n",
    "    return sentences_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Write word frequencies to CSV\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    \"\"\"\n",
    "    Writes the word frequencies to the specified CSV file.\n",
    "    \"\"\"\n",
    "    # Sort the word frequencies by frequency in descending order\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    with open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Word', 'Frequency'])  # Write header row\n",
    "\n",
    "        for word, frequency in sorted_word_freq:\n",
    "            writer.writerow([word, frequency])\n",
    "    \n",
    "    print(f\"Word frequencies written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the workflow\n",
    "def main():\n",
    "    # Step 1: Remove header from the input CSV and create a new file without it\n",
    "    remove_header(input_filename, final_filename)\n",
    "\n",
    "    # Step 2: Extract and clean sentences from the text\n",
    "    sentences_clean = extract_sentences(final_filename)\n",
    "\n",
    "    # Step 3: Calculate word frequencies\n",
    "    word_frequencies = Counter(sentences_clean)\n",
    "    #calculate_word_frequencies(sentences_clean)\n",
    "\n",
    "    # Step 4: Write word frequencies to CSV\n",
    "    write_word_frequencies(word_frequencies, word_freq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Economic Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('A_word_freq.csv')\n",
    "\n",
    "df['Rank'] = df.index + 1\n",
    "\n",
    "df['log_rank'] = np.log(df['Rank'])\n",
    "df['log_freq'] = np.log(df['Frequency'])\n",
    "\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df['log_rank'])\n",
    "\n",
    "# Create the OLS model\n",
    "model = sm.OLS(df['log_freq'], X)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = results.params['const']  # Intercept (the constant)\n",
    "slope = results.params['log_rank']          # Slope (coefficient for x)\n",
    "\n",
    "# Extract the R-squared value\n",
    "r_squared = results.rsquared\n",
    "\n",
    "# Extract the p-value for the slope\n",
    "p_value = results.pvalues['log_rank']\n",
    "\n",
    "conf_intervals = results.conf_int()\n",
    "\n",
    "t_value = results.tvalues['log_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results\n",
    "print(f\"Slope (Zipf's Law exponent s): {slope}\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(f\"T-value: {t_value}\")\n",
    "print(f\"Confidence Intervals:\\n{conf_intervals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 100 Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100 = df[:100].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_rank_end = df_100['log_rank'].iloc[99]  # log rank at rank 100\n",
    "log_freq_end = df_100['log_freq'].iloc[99]\n",
    "# Normalize all log frequencies by the log frequency at rank 100\n",
    "df_100['norm_log_freq'] = df_100['log_freq'] - log_freq_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = sm.add_constant(df_100['log_rank'])\n",
    "\n",
    "# Create the OLS model\n",
    "model4 = sm.OLS(df_100['norm_log_freq'], X4)\n",
    "\n",
    "# Fit the model\n",
    "results4 = model4.fit()\n",
    "\n",
    "intercept4 = results4.params['const']  # Intercept (the constant)\n",
    "slope4 = results4.params['log_rank']  \n",
    "\n",
    "print(f\"Slope (Zipf's Law Slope s): {slope4}\")\n",
    "print(f\"Intercept: {intercept4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_100['log_rank'], df_100['norm_log_freq'], label='Data', color='blue')\n",
    "\n",
    "#plt.plot(df_100['log_rank'], intercept1 + slope1 * df_100['log_rank'], 'r', label='Fit: f(r) ‚àù 1/r^s')\n",
    "\n",
    "\n",
    "x_vals = df_100['log_rank']\n",
    "y_vals = df_100['norm_log_freq'].iloc[-1] - (x_vals - log_rank_end)  # since slope is -1\n",
    "\n",
    "plt.plot(x_vals, y_vals, 'g--', label=f'Theoretical Slope = -1')\n",
    "\n",
    "plt.xlabel('Log Rank')\n",
    "plt.ylabel('Log Normalized Frequency')\n",
    "plt.title('Zipf\\'s Law Fit for Most Common 100 Words: Empirical vs Theoretical Slope')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequencies (with car replacement, no stop words, by cell/text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import io\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variables that might change when I switch from sample data to scraped data\n",
    "\n",
    "data = 'edmunds_comments.csv'\n",
    "text_row = 2 - 1\n",
    "output_file = 'B_word_freq.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dictionary from the car brand and model csv\n",
    "\n",
    "df = pd.read_csv('car_models_and_brands.csv')\n",
    "model_to_brand = dict(zip(df['Model'], df['Brand']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean and tokenize - here I'm taking out stop words and replacing model with brand as well\n",
    "\n",
    "#Download and initialize stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_and_tokenize (sentence):\n",
    "    #first we're going to remove punctuation\n",
    "    no_punct = re.sub(f'[{re.escape(string.punctuation)}]', ' ', sentence)\n",
    "    #now make lowercase\n",
    "    lowered = no_punct.lower()\n",
    "    #take out possessive punctuation\n",
    "    for original, replacement in model_to_brand.items():\n",
    "        lowered = re.sub(rf'\\b{re.escape(original)}\\'?s?\\b', replacement, lowered, flags=re.IGNORECASE)\n",
    "    #split the sentence into words\n",
    "    words = lowered.split()\n",
    "    #replace car models with brand names and remove stop words\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word in model_to_brand:\n",
    "            cleaned_words.append(model_to_brand[word])  # Replace model with brand\n",
    "        elif word not in stop_words:\n",
    "            cleaned_words.append(word)  # Keep the word if it's not a stop word\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove header, but I'm not actually using it\n",
    "\n",
    "def remove_header(input, output):\n",
    "    with open(input, 'r', newline = '') as header_file:\n",
    "        reader = csv.reader(header_file)\n",
    "        next(reader)\n",
    "        #Now let's write the output file\n",
    "        with open(output, 'w', newline='') as no_header_file:\n",
    "            writer = csv.writer(no_header_file)\n",
    "            for row in reader:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract posts and clean them\n",
    "\n",
    "def extract_text(file):\n",
    "\n",
    "    text_clean = []\n",
    "    with open(file, 'r', newline='', encoding='utf-8') as data_file:\n",
    "        reader = csv.reader(data_file)\n",
    "        # Skip the header\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            text = row[text_row]\n",
    "            cleaned_tokens = clean_and_tokenize(text)\n",
    "            if cleaned_tokens:  # Avoid adding empty sentences\n",
    "                text_clean.append(cleaned_tokens)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate word frequencies; counted once per post\n",
    "\n",
    "def calculate_word_frequencies(text_list):\n",
    "    frequencies = {}\n",
    "    for text in text_list:\n",
    "        unique_words = set(text)\n",
    "        for word in unique_words:\n",
    "            if word in frequencies:\n",
    "                frequencies[word] += 1\n",
    "            else:\n",
    "                frequencies[word] = 1       \n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now write the frequencies to a csv\n",
    "\n",
    "def write_word_frequencies(word_freq, output_file):\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as freq_file:\n",
    "        writer = csv.writer(freq_file)\n",
    "        #Write the header row\n",
    "        writer.writerow(['Word', 'Frequency'])\n",
    "        for word, freq in word_freq.items():\n",
    "            writer.writerow([word, freq])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    cleaned_text = extract_text(data)\n",
    "    freq_dict = calculate_word_frequencies(cleaned_text)\n",
    "    write_word_frequencies(freq_dict, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find Top Ten Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n",
    "df_brands = pd.read_csv('car_models_and_brands.csv')\n",
    "\n",
    "#let's get just the brands so we can filter the words df\n",
    "df_brands = df_brands[df_brands['Brand'] != 'car']\n",
    "df_brands = df_brands[df_brands['Brand'] != 'seat']\n",
    "df_brands = df_brands[df_brands['Brand'] != 'sedan']\n",
    "df_brands = df_brands[df_brands['Brand'] != 'problem']\n",
    "df_brands = df_brands['Brand'].unique()\n",
    "\n",
    "brand_list = list(df_brand_freq['Word'])\n",
    "brand_list\n",
    "\n",
    "#no filter for brand frequency\n",
    "df_brand_freq = df[df['Word'].isin(df_brands)]\n",
    "df_brand_freq = df_brand_freq.sort_values(by = 'Frequency', ascending = False)[:10].reset_index(drop = True)\n",
    "df_brand_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "# Read the CSV file into a dataframe\n",
    "df = pd.read_csv('edmunds_comments.csv')\n",
    "\n",
    "# Load the car models and brands CSV into a dataframe\n",
    "df_brands = pd.read_csv('car_models_and_brands.csv')\n",
    "\n",
    "# Create a dictionary for faster lookup, where keys are the models and values are the brands\n",
    "model_to_brand = dict(zip(car_brands_df['Model'], car_brands_df['Brand']))\n",
    "\n",
    "df['Comment'] = df['Comment'].astype(str)\n",
    "\n",
    "##########################################\n",
    "######### PREPROCESSING FUNCTION #########\n",
    "##########################################\n",
    "\n",
    "def preprocess_Comment(Comment):\n",
    "    if isinstance(Comment, str):  # Only process if Comment is a string\n",
    "        Comment = Comment.lower()  # convert to lowercase\n",
    "        Comment = Comment.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))  # replace punctuation with spaces\n",
    "        Comment = ' '.join(Comment.split())  # remove extra spaces\n",
    "    return Comment\n",
    "\n",
    "# Apply preprocessing to the 'Comment' column\n",
    "df['Comment'] = df['Comment'].apply(preprocess_Comment)\n",
    "\n",
    "##########################################\n",
    "######### REPLACE MODEL NAMES ###########\n",
    "##########################################\n",
    "\n",
    "# Function to replace model names in the 'Comment' column with their associated brands\n",
    "# Ensures the model name is replaced only if it's a standalone word\n",
    "def replace_model_with_brand(Comment):\n",
    "    if isinstance(Comment, str):  # Only process if Comment is a string\n",
    "        for model, brand in model_to_brand.items():\n",
    "            # Use word boundaries (\\b) to ensure the model name is not part of a longer word\n",
    "            Comment = re.sub(r'\\b{}\\b'.format(re.escape(model.lower())), brand.lower(), Comment)\n",
    "    return Comment\n",
    "\n",
    "# Apply the function to the 'Comment' column after preprocessing\n",
    "df['Comment'] = df['Comment'].apply(replace_model_with_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "######## CALCULATE LIFT ##########\n",
    "##################################\n",
    "\n",
    "brand_list = [brand.lower() for brand in brand_list]\n",
    "\n",
    "def brand_mentioned(Comment, brand):\n",
    "    # Use regular expressions to find whole words, ignoring punctuation\n",
    "    return bool(re.search(rf'\\b{re.escape(brand)}\\b', Comment))\n",
    "\n",
    "# Function to check if two brands are mentioned within 15 words of each other\n",
    "def brands_within_15_words(Comment, brand1, brand2):\n",
    "    words = Comment.split()\n",
    "    brand1_indices = [i for i, word in enumerate(words) if word == brand1]\n",
    "    brand2_indices = [i for i, word in enumerate(words) if word == brand2]\n",
    "    for i in brand1_indices:\n",
    "        for j in brand2_indices:\n",
    "            if abs(i - j) <= 15:  # Check if brands are within 7 words\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "# Dictionary to store the lift values\n",
    "lift_values = {}\n",
    "\n",
    "# Calculate lift for each pair of brands\n",
    "total_Comments = len(df)\n",
    "\n",
    "for brand1, brand2 in itertools.combinations(brand_list, 2):\n",
    "    # Count the number of Comments that mention brand1 and brand2 within 15 words of each other\n",
    "    Comments_with_both = df['Comment'].apply(lambda Comment: brands_within_15_words(Comment, brand1, brand2)).sum()\n",
    "    \n",
    "    # Count the number of Comments that mention each brand (at least once)\n",
    "    Comments_with_brand1 = df['Comment'].apply(lambda Comment: brand_mentioned(Comment, brand1)).sum()\n",
    "    Comments_with_brand2 = df['Comment'].apply(lambda Comment: brand_mentioned(Comment, brand2)).sum()\n",
    "\n",
    "    # print(f\"Comments with {brand1}: {Comments_with_brand1}\")\n",
    "    # print(f\"Comments with {brand2}: {Comments_with_brand2}\")\n",
    "    # print(f\"Comments with {brand1} and {brand2}: {Comments_with_both}\")\n",
    "    # print()\n",
    "    \n",
    "    # Calculate the lift\n",
    "    if Comments_with_brand1 > 0 and Comments_with_brand2 > 0:\n",
    "        lift = (total_Comments * Comments_with_both) / (Comments_with_brand1 * Comments_with_brand2)\n",
    "        lift_values[f'{brand1}-{brand2}'] = lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort the dictionary by lift values (smallest to largest)\n",
    "# sorted_lift_values = dict(sorted(lift_values.items(), key=lambda item: item[1]))\n",
    "\n",
    "# # Print the sorted lift values\n",
    "# for brand_pair, lift in sorted_lift_values.items():\n",
    "#     print(f'{brand_pair}: {lift}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Create an empty DataFrame for the lift values, with brands as rows and columns\n",
    "lift_df = pd.DataFrame(index=brand_list, columns=brand_list)\n",
    "\n",
    "# Populate the DataFrame with lift values\n",
    "for key, lift_value in lift_values.items():\n",
    "    brand1, brand2 = key.split('-')  # Split the key into brand1 and brand2\n",
    "    lift_df.loc[brand1, brand2] = lift_value\n",
    "    lift_df.loc[brand2, brand1] = lift_value  # Since lift is symmetric\n",
    "\n",
    "# Fill the diagonal with NaN or 0 since a brand compared to itself isn't meaningful\n",
    "np.fill_diagonal(lift_df.values, np.nan)\n",
    "\n",
    "# Convert to numeric type\n",
    "lift_df = lift_df.astype(float)\n",
    "\n",
    "# Create a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(lift_df, dtype=bool))\n",
    "\n",
    "# Define a custom colormap that shifts from red (<1) to blue (>1) with a gradient scale\n",
    "cmap = LinearSegmentedColormap.from_list('custom_cmap', ['red', 'white', 'blue'], N=256)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(lift_df, annot=True, fmt=\".3f\", cmap=cmap, linewidths=.5, mask=mask, cbar_kws={'label': 'Lift Value'}, center=1)\n",
    "plt.title(\"Lift Values Between Brands\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "distance_matrix = np.zeros((len(brand_list), len(brand_list)))\n",
    "\n",
    "for i, brand1 in enumerate(brand_list):\n",
    "    for j, brand2 in enumerate(brand_list):\n",
    "        if i != j:\n",
    "            # Get the lift value if it exists, otherwise use a default high distance (e.g., 1e6)\n",
    "            lift_value = lift_values.get(f'{brand1}-{brand2}') or lift_values.get(f'{brand2}-{brand1}') or 1e6\n",
    "            distance_matrix[i, j] = 1 / lift_value  # Inverse of lift as distance\n",
    "\n",
    "# Apply Multi-Dimensional Scaling (MDS)\n",
    "mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "mds_coordinates = mds.fit_transform(distance_matrix)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter plot with markers for each brand\n",
    "plt.scatter(mds_coordinates[:, 0], mds_coordinates[:, 1], s=100, color='blue')\n",
    "\n",
    "# List to store text objects for adjustment\n",
    "texts = []\n",
    "\n",
    "# Annotate points with brand names, and store text objects for adjustment\n",
    "for i, brand in enumerate(brand_list):\n",
    "    texts.append(plt.text(mds_coordinates[i, 0], mds_coordinates[i, 1], brand, fontsize=12, ha='right',\n",
    "                          bbox=dict(facecolor='white', alpha=0.7)))\n",
    "\n",
    "# Adjust the positions of the text labels to avoid overlap, add arrows pointing to the actual points\n",
    "adjust_text(texts, x=mds_coordinates[:, 0], y=mds_coordinates[:, 1],\n",
    "            arrowprops=dict(arrowstyle=\"->\", color='gray', lw=1))\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"MDS Map of Brands Based on Lift\", fontsize=16)\n",
    "plt.xlabel(\"MDS Dimension 1\", fontsize=14)\n",
    "plt.ylabel(\"MDS Dimension 2\", fontsize=14)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Stress: {mds.stress_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plots\n",
    "\n",
    "# Create the distance matrix\n",
    "distance_matrix = np.zeros((len(brand_list), len(brand_list)))\n",
    "\n",
    "for i, brand1 in enumerate(brand_list):\n",
    "    for j, brand2 in enumerate(brand_list):\n",
    "        if i != j:\n",
    "            # Get the lift value if it exists, otherwise use a default high distance (e.g., 1e6)\n",
    "            lift_value = lift_values.get(f'{brand1}-{brand2}') or lift_values.get(f'{brand2}-{brand1}') or 1e6\n",
    "            distance_matrix[i, j] = 1 / lift_value  # Inverse of lift as distance\n",
    "\n",
    "# Apply Multi-Dimensional Scaling (MDS) with 3 components\n",
    "mds = MDS(n_components=3, dissimilarity='precomputed', random_state=42)\n",
    "mds_coordinates = mds.fit_transform(distance_matrix)\n",
    "\n",
    "# Plot the results in 3D\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot with markers for each brand in 3D\n",
    "ax.scatter(mds_coordinates[:, 0], mds_coordinates[:, 1], mds_coordinates[:, 2], s=100, color='blue')\n",
    "\n",
    "# Annotate points with brand names\n",
    "for i, brand in enumerate(brand_list):\n",
    "    ax.text(mds_coordinates[i, 0], mds_coordinates[i, 1], mds_coordinates[i, 2], brand, fontsize=12)\n",
    "\n",
    "# Add title and axis labels\n",
    "ax.set_title(\"3D MDS Map of Brands Based on Lift\", fontsize=16)\n",
    "ax.set_xlabel(\"MDS Dimension 1\", fontsize=14)\n",
    "ax.set_ylabel(\"MDS Dimension 2\", fontsize=14)\n",
    "ax.set_zlabel(\"MDS Dimension 3\", fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Task H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
